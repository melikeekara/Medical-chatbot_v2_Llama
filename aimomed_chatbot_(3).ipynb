{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Gerekli Kurulumlar"
      ],
      "metadata": {
        "id": "2tT1QF0qaGJ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlW6PeKvYAAN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes peft --quiet streamlit\n",
        "!pip install python-dotenv\n",
        "!npm install -g localtunnel\n",
        "!pip install PyPDF2 faiss-cpu sentence-transformers langchain\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain-community faiss-cpu PyPDF2 sentence-transformers"
      ],
      "metadata": {
        "id": "fB0s0rxV1UTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
        "from datetime import datetime\n",
        "from peft import PeftModel\n",
        "import matplotlib.pyplot as plt\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "MHxeBIktaYUF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xRFs4Y39bAvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "Q5rnj-r2bFGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Token Giri≈üi (Eƒüer gereklirse)\n",
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ],
      "metadata": {
        "id": "eUdduK3DYbrX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import shelve\n",
        "import re\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Logo path\n",
        "HORIZONTAL_RED = \"/content/drive/MyDrive/aƒ±_med.png\"\n",
        "\n",
        "# Sayfa ayarƒ±\n",
        "st.set_page_config(page_title=\"AIMO MED\", page_icon=\"ü©∫\")\n",
        "st.title(\"ü©∫ AIMO MED\")\n",
        "\n",
        "# --- Sidebar ---\n",
        "with st.sidebar:\n",
        "    st.image(HORIZONTAL_RED, use_container_width=True)\n",
        "    lang = st.selectbox(\"üåê Dil Se√ßimi / Language:\", [\"T√ºrk√ße\", \"English\"])\n",
        "    st.header(\"üìÅ Ge√ßmi≈ü Sohbetler\" if lang == \"T√ºrk√ße\" else \"üìÅ Chat History\")\n",
        "\n",
        "st.markdown(\n",
        "    \"Bu chatbot, T√ºrk√ße ve ƒ∞ngilizce tƒ±bbi sorularƒ±nƒ±za yardƒ±mcƒ± olmak i√ßin eƒüitildi. Sorularƒ±nƒ±zƒ± yazabilirsiniz.\"\n",
        "    if lang == \"T√ºrk√ße\"\n",
        "    else \"This chatbot is trained to assist with medical questions in both Turkish and English. You can ask your questions.\"\n",
        ")\n",
        "\n",
        "def create_new_chat_id():\n",
        "    return datetime.now().strftime(\"chat_%Y%m%d_%H%M%S\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"chat_key\" not in st.session_state:\n",
        "    st.session_state.chat_key = create_new_chat_id()\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model_name = \"OnurYantira/llama3-8b-turkish-english-medical-merged\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "def list_all_chats():\n",
        "    with shelve.open(\"chat_history\") as db:\n",
        "        return list(db.keys())\n",
        "\n",
        "def load_specific_chat(chat_key):\n",
        "    with shelve.open(\"chat_history\") as db:\n",
        "        return db.get(chat_key, [])\n",
        "\n",
        "def save_specific_chat(chat_key, messages):\n",
        "    with shelve.open(\"chat_history\") as db:\n",
        "        db[chat_key] = messages\n",
        "\n",
        "def extract_keywords(text, num_words=3):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return \" \".join(words[:num_words])\n",
        "\n",
        "def format_chat_title(chat_key, messages):\n",
        "    if messages and messages[0][\"role\"] == \"user\":\n",
        "        summary = extract_keywords(messages[0][\"content\"])\n",
        "        return summary.capitalize()\n",
        "    try:\n",
        "        dt = datetime.strptime(chat_key, \"chat_%Y%m%d_%H%M%S\")\n",
        "        return dt.strftime(\"%d %B %Y - %H:%M\")\n",
        "    except:\n",
        "        return chat_key\n",
        "\n",
        "with st.sidebar:\n",
        "    all_chats = list_all_chats()\n",
        "    chat_titles = [format_chat_title(chat, load_specific_chat(chat)) for chat in all_chats]\n",
        "\n",
        "    cols = st.columns([1, 1])\n",
        "    if cols[0].button(\"‚ûï Yeni Sohbet\" if lang == \"T√ºrk√ße\" else \"‚ûï New Chat\"):\n",
        "        new_key = create_new_chat_id()\n",
        "        st.session_state.chat_key = new_key\n",
        "        st.session_state.messages = []\n",
        "        save_specific_chat(new_key, [])\n",
        "        st.rerun()\n",
        "\n",
        "    if cols[1].button(\"üóë Sohbeti Sil\" if lang == \"T√ºrk√ße\" else \"üóë Delete Chat\"):\n",
        "        with shelve.open(\"chat_history\") as db:\n",
        "            if st.session_state.chat_key in db:\n",
        "                del db[st.session_state.chat_key]\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "\n",
        "    if chat_titles:\n",
        "        st.markdown(\"### Sohbeti se√ßin:\" if lang == \"T√ºrk√ße\" else \"### Select a chat:\")\n",
        "        for i, title in enumerate(chat_titles):\n",
        "            if st.button(title, key=f\"chat_{i}\"):\n",
        "                selected_index = i\n",
        "                chat_key = all_chats[selected_index]\n",
        "                st.session_state.chat_key = chat_key\n",
        "                st.session_state.messages = load_specific_chat(chat_key)\n",
        "                st.rerun()\n",
        "\n",
        "# Stil\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        ".chat-container { display: flex; margin: 10px 0; align-items: flex-start; }\n",
        ".chat-left { flex-direction: row; }\n",
        ".chat-right { flex-direction: row-reverse; justify-content: flex-end; }\n",
        ".avatar { font-size: 24px; margin: 0 10px; }\n",
        ".chat-bubble { max-width: 75%; padding: 12px 16px; border-radius: 18px; line-height: 1.5; word-wrap: break-word; color: #000; background-color: #f0f2f6; border: 1px solid #d1d5db; }\n",
        ".user-msg { background-color: #e3f2fd; color: #000; }\n",
        ".bot-msg { background-color: #f1f8e9; color: #000; }\n",
        "@media (prefers-color-scheme: dark) {\n",
        "  .chat-bubble { color: #f9f9f9 !important; background-color: #333 !important; border: 1px solid #555 !important; }\n",
        "  .user-msg { background-color: #1a73e8 !important; color: white !important; }\n",
        "  .bot-msg { background-color: #1e8e3e !important; color: white !important; }\n",
        "}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Sohbetleri g√∂ster\n",
        "for message in st.session_state.messages:\n",
        "    role = message[\"role\"]\n",
        "    content = message[\"content\"]\n",
        "    avatar = \"üë§\" if role == \"user\" else \"ü§ñ\"\n",
        "    css_class = \"user-msg\" if role == \"user\" else \"bot-msg\"\n",
        "    align_class = \"chat-left\" if role == \"user\" else \"chat-right\"\n",
        "\n",
        "    st.markdown(f'''\n",
        "        <div class=\"chat-container {align_class}\">\n",
        "            <div class=\"avatar\">{avatar}</div>\n",
        "            <div class=\"chat-bubble {css_class}\">{content}</div>\n",
        "        </div>\n",
        "    ''', unsafe_allow_html=True)\n",
        "\n",
        "user_input = st.chat_input(\"Bir ≈üey yazƒ±n...\" if lang == \"T√ºrk√ße\" else \"Type something...\")\n",
        "\n",
        "if user_input:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    system_prompt = (\n",
        "        \"Sen T√ºrk√ße konu≈üan yardƒ±mcƒ± bir medikal asistansƒ±n. Kƒ±sa, doƒüru ve profesyonel yanƒ±tlar ver.\"\n",
        "        if lang == \"T√ºrk√ße\" else\n",
        "        \"You are a helpful medical assistant. Provide concise, accurate, and professional answers.\"\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
        "        f\"{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
        "        f\"{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    response = generated_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\")[-1].strip()\n",
        "\n",
        "    response = re.sub(rf\"{re.escape(user_input)}\", \"\", response).strip()\n",
        "    response = re.sub(r\"(<\\|.*?\\|>|system|user|assistant)\", \"\", response, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "    save_specific_chat(st.session_state.chat_key, st.session_state.messages)\n",
        "    st.rerun()\n"
      ],
      "metadata": {
        "id": "gByzPaRXaS_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword\n"
      ],
      "metadata": {
        "id": "OsePX21_boX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "_Tnw6kRBbrcF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}